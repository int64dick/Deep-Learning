為何學習神經網路?
  神經網路能夠自動學習適當權重。
  
  
神經網路與感知器:
  當輸入訊號經過權重相加後來到節點,有別於感知器的if elif,神經網路出現了活化函數,活化函數會再將權重總和帶入活化函數中,生成輸出訊號。
  而這活化函數正是感知器踏入神經網路的橋梁。
  

活化函數:
  原先感知器所使用的也是活化函數,但採用的是其中的階梯函數,若將階梯函數改成其他函數時,便邁入了神經網路的領域。
活化函數例子:
  sigmoid函數:
  def sigmoid(x):
    return 1/(1+np.exp(-x))
    
  階梯函數:
  def step_function(x):
    return np.array(x>0, dtype = np.int)
    
  而sigmoid函數與階梯函數從廣義上是相似的,皆具有輸入較小及較大,輸出則較小及較大,輸出在0和1之間和皆為非線性函數等性質。
  然而sigmoid函數所擁有的平滑性是極具重要的。
  
  
為何活化函數我們總是討論非線性函數?
  因為若活化函數為線性函數,其層疊將會完全失去意義。
  經過兩次線性函數疊加後的神經網路y(x) = h(h(x)) = c*c*x 與一次線性函數y(x) = ax(但a = c*c) 是一樣結果。
  
  ReLU函數:
  def ReLU(x):
    return np.maximum(0, x)
    
  ReLU函數也是常見的活化函數一種。
  
  
多維陣列運算:
  將輸入乘上權重後相加,就某種意義上即是矩陣乘法,我們利用矩陣乘法來完成想要的意義,所以熟練多維陣列之間的運算是非常必要的!!
  
  
輸出層的設計:
  前面探討了活化函數使用線性會使得層疊毫無意義,但當我們來到最後一層節點(輸出層)時,
我們會使用恆等函數與softmax函數(雖然是線性函數,但因為是最後一層,所以仍具有疊加意義)

  一般而言,分類問題採用的是softmax函數,迴歸問題則使用恆等函數。
  
  
  

  
